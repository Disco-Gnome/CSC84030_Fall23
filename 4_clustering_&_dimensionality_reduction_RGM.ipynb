{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Disco-Gnome/CSC84030_Fall23/blob/main/4_clustering_%26_dimensionality_reduction_RGM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b2d030-e649-4c98-a56f-9b0eab9bdf75"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=6bc4005e99e05816e8df003d1f70d2e1c67efbed8439108dc4c5432c9a5c4263\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 120875 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "Now we import some of the libraries usually needed by our workload."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twk-K-jilWK7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtrJlMBt1Ela"
      },
      "source": [
        "Let's initialize the Spark context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm3sAVeK1EDZ"
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYRX2PMm0L6"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hXdMR6wnEIM"
      },
      "source": [
        "In this Colab, we will load a famous machine learning dataset, the [Breast Cancer Wisconsin dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), using the ```scikit-learn``` datasets loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K93ABEy9Zlo"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast_cancer = load_breast_cancer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpsaYOqRxar2"
      },
      "source": [
        "For convenience, given that the dataset is small, we first construct a Pandas dataframe, tune the schema, and then convert it into a Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oitav_xhQD9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a19ab49-0975-45ce-c28b-7357343fb777",
        "cellView": "code"
      },
      "source": [
        "pd_df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "df = spark.createDataFrame(pd_df)\n",
        "\n",
        "def set_df_columns_nullable(spark, df, column_list, nullable=False):\n",
        "    for struct_field in df.schema:\n",
        "        if struct_field.name in column_list:\n",
        "            struct_field.nullable = nullable\n",
        "    df_mod = spark.createDataFrame(df.rdd, df.schema)\n",
        "    return df_mod\n",
        "\n",
        "df = set_df_columns_nullable(spark, df, df.columns)\n",
        "df = df.withColumn('features', array(df.columns))\n",
        "vectors = df.rdd.map(lambda row: Vectors.dense(row.features))\n",
        "\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mean radius: double (nullable = false)\n",
            " |-- mean texture: double (nullable = false)\n",
            " |-- mean perimeter: double (nullable = false)\n",
            " |-- mean area: double (nullable = false)\n",
            " |-- mean smoothness: double (nullable = false)\n",
            " |-- mean compactness: double (nullable = false)\n",
            " |-- mean concavity: double (nullable = false)\n",
            " |-- mean concave points: double (nullable = false)\n",
            " |-- mean symmetry: double (nullable = false)\n",
            " |-- mean fractal dimension: double (nullable = false)\n",
            " |-- radius error: double (nullable = false)\n",
            " |-- texture error: double (nullable = false)\n",
            " |-- perimeter error: double (nullable = false)\n",
            " |-- area error: double (nullable = false)\n",
            " |-- smoothness error: double (nullable = false)\n",
            " |-- compactness error: double (nullable = false)\n",
            " |-- concavity error: double (nullable = false)\n",
            " |-- concave points error: double (nullable = false)\n",
            " |-- symmetry error: double (nullable = false)\n",
            " |-- fractal dimension error: double (nullable = false)\n",
            " |-- worst radius: double (nullable = false)\n",
            " |-- worst texture: double (nullable = false)\n",
            " |-- worst perimeter: double (nullable = false)\n",
            " |-- worst area: double (nullable = false)\n",
            " |-- worst smoothness: double (nullable = false)\n",
            " |-- worst compactness: double (nullable = false)\n",
            " |-- worst concavity: double (nullable = false)\n",
            " |-- worst concave points: double (nullable = false)\n",
            " |-- worst symmetry: double (nullable = false)\n",
            " |-- worst fractal dimension: double (nullable = false)\n",
            " |-- features: array (nullable = false)\n",
            " |    |-- element: double (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtR1xRvonxiO"
      },
      "source": [
        "With the next cell, we build the two data structures that we will be using throughout this Colab:\n",
        "\n",
        "\n",
        "*   ```features```, a dataframe of Dense vectors, containing all the original features in the dataset;\n",
        "*   ```labels```, a series of binary labels indicating if the corresponding set of features belongs to a subject with breast cancer, or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP23Xkgwi0SD"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "features = spark.createDataFrame(vectors.map(Row), [\"features\"])\n",
        "labels = pd.Series(breast_cancer.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For debugging!\n",
        "# features is a dataframe of just the features column, where each value is a dense vector of ...\n",
        "# TODO: why is this not a list of strings?\n",
        "print('features meta data: ', features)\n",
        "print('features: ', features.show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OozZ-vSAZPD-",
        "outputId": "6bab4628-b372-4c1b-fd16-8e2e1cf2888b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features meta data:  DataFrame[features: vector]\n",
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|[17.99,10.38,122....|\n",
            "|[20.57,17.77,132....|\n",
            "|[19.69,21.25,130....|\n",
            "|[11.42,20.38,77.5...|\n",
            "|[20.29,14.34,135....|\n",
            "|[12.45,15.7,82.57...|\n",
            "|[18.25,19.98,119....|\n",
            "|[13.71,20.83,90.2...|\n",
            "|[13.0,21.82,87.5,...|\n",
            "|[12.46,24.04,83.9...|\n",
            "|[16.02,23.24,102....|\n",
            "|[15.78,17.89,103....|\n",
            "|[19.17,24.8,132.4...|\n",
            "|[15.85,23.95,103....|\n",
            "|[13.73,22.61,93.6...|\n",
            "|[14.54,27.54,96.7...|\n",
            "|[14.68,20.13,94.7...|\n",
            "|[16.13,20.68,108....|\n",
            "|[19.81,22.15,130....|\n",
            "|[13.54,14.36,87.4...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "features:  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7"
      },
      "source": [
        "### Your task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLNUxP0_8x3"
      },
      "source": [
        "If you run successfully the Setup and Data Preprocessing stages, you are now ready to cluster the data with the [K-means](https://spark.apache.org/docs/latest/ml-clustering.html) algorithm included in MLlib (Spark's Machine Learning library).\n",
        "Set the ```k``` parameter to **2**, fit the model, and the compute the [Silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)) (i.e., a measure of quality of the obtained clustering).  \n",
        "\n",
        "**IMPORTANT:** use the MLlib implementation of the Silhouette score (via ```ClusteringEvaluator```)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xVIfPHZwWaE"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Code referenced from: https://spark.apache.org/docs/latest/ml-clustering.html#output-columns\n",
        "\n",
        "# Trains a k-means model.\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(df)\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"labels variable (i.e., the ground truth from our dataset) =\\n\", labels)\n",
        "print(\"Silhouette with squared euclidean distance = \", silhouette)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Yh-JKSoKkek",
        "outputId": "45025129-aed2-4228-85ee-4668e9d8332a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels variable (i.e., the ground truth from our dataset) =\n",
            " 0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "564    0\n",
            "565    0\n",
            "566    0\n",
            "567    0\n",
            "568    1\n",
            "Length: 569, dtype: int64\n",
            "Silhouette with squared euclidean distance =  0.8342904262826145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GB09n7sqTO6"
      },
      "source": [
        "Take the predictions produced by K-means, and compare them with the ```labels``` variable (i.e., the ground truth from our dataset).  \n",
        "\n",
        "Compute how many data points in the dataset have been clustered correctly (i.e., positive cases in one cluster, negative cases in the other).\n",
        "\n",
        "*HINT*: you can use ```np.count_nonzero(series_a == series_b)``` to quickly compute the element-wise comparison of two series.\n",
        "\n",
        "**IMPORTANT**: K-means is a clustering algorithm, so it will not output a label for each data point, but just a cluster identifier!  As such, label ```0``` does not necessarily match the cluster identifier ```0```.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Is this comparing values correctly? Is 'prediction' the correct feature to select?\n",
        "#Could the binary values be flipped? aka is this 83 RIGHT or 83 WRONG?"
      ],
      "metadata": {
        "id": "dZ2SgfhIZ7B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQhC3APIPPM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d3c90e-62ee-4efa-e4aa-37c8b2cbf57f"
      },
      "source": [
        "prediction_series = (predictions.select('prediction').rdd.map(lambda x: x[0]).collect())\n",
        "np.count_nonzero(labels == prediction_series)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLIprM1JsdTU"
      },
      "source": [
        "Now perform dimensionality reduction on the ```features``` using [SVD](https://spark.apache.org/docs/latest/mllib-dimensionality-reduction), available as well in MLlib.\n",
        "\n",
        "Reduce the dimensionality to **2**, effectively reducing the dataset size of a **15X** factor. Name the new dataset as ```svdFeatures```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4J8JMDkSb24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581682b2-97e0-474f-8c07-85e545741e63"
      },
      "source": [
        "from pyspark.mllib.linalg.distributed import RowMatrix\n",
        "\n",
        "# Referenced from: https://stackoverflow.com/a/53066752\n",
        "# After Google search: turn dataframe into a rowmatrix\n",
        "# features is a DataFrame\n",
        "featuresRDD = features.rdd.map(list)\n",
        "featuresRowMatrix = RowMatrix(featuresRDD)\n",
        "# Referenced from: https://spark.apache.org/docs/latest/mllib-dimensionality-reduction#svd-example\n",
        "svdFeatures = featuresRowMatrix.computeSVD(2, computeU=True)\n",
        "print('The U factor is a RowMatrix: \\n', svdFeatures.U)\n",
        "print('The singular values are stored in a local dense vector: \\n', svdFeatures.s)\n",
        "print('The V factor is a local dense matrix:')\n",
        "print(svdFeatures.V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The U factor is a RowMatrix: \n",
            " <pyspark.mllib.linalg.distributed.RowMatrix object at 0x7d9f8a125f00>\n",
            "The singular values are stored in a local dense vector: \n",
            " [30786.444627835797,2480.4457833852953]\n",
            "The V factor is a local dense matrix:\n",
            "DenseMatrix([[ 1.07417853e-02,  3.10857421e-02],\n",
            "             [ 1.34045777e-02,  4.83124253e-02],\n",
            "             [ 7.04506088e-02,  1.97364828e-01],\n",
            "             [ 5.72522445e-01,  7.70224130e-01],\n",
            "             [ 6.51751678e-05,  2.62097172e-04],\n",
            "             [ 8.01017182e-05,  1.75341873e-04],\n",
            "             [ 8.07639302e-05,  4.07661495e-05],\n",
            "             [ 4.51934799e-05,  1.69018436e-05],\n",
            "             [ 1.22298430e-04,  4.97684199e-04],\n",
            "             [ 4.10463505e-05,  1.84499403e-04],\n",
            "             [ 3.52395083e-04,  8.41315666e-05],\n",
            "             [ 7.91756415e-04,  4.03047670e-03],\n",
            "             [ 2.49763979e-03,  1.33910533e-03],\n",
            "             [ 4.15710660e-02, -6.79661807e-02],\n",
            "             [ 4.46726431e-06,  2.42657052e-05],\n",
            "             [ 1.84034785e-05,  6.56841723e-05],\n",
            "             [ 2.35606720e-05,  8.35322707e-05],\n",
            "             [ 8.71328141e-06,  3.01365028e-05],\n",
            "             [ 1.33301479e-05,  7.25209655e-05],\n",
            "             [ 2.50009455e-06,  1.16619632e-05],\n",
            "             [ 1.27106222e-02,  2.41432752e-02],\n",
            "             [ 1.78787301e-02,  5.79087644e-02],\n",
            "             [ 8.44072509e-02,  1.54087672e-01],\n",
            "             [ 8.10936848e-01, -5.76353692e-01],\n",
            "             [ 8.96729192e-05,  3.22213232e-04],\n",
            "             [ 1.95800247e-04,  2.65381775e-04],\n",
            "             [ 2.25664038e-04,  1.46636756e-04],\n",
            "             [ 9.56995686e-05,  7.94819348e-05],\n",
            "             [ 1.97533332e-04,  7.03619455e-04],\n",
            "             [ 5.62166955e-05,  2.08881378e-04]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert svd.U to dataframe\n",
        "# svdFeatUrows = svdFeatures.U.rows.map(lambda x: (x[0],x[1])).collect()\n",
        "# reduced_df = spark.createDataFrame(pd.DataFrame(svdFeatUrows))\n",
        "# # Set nullable to prevent error in kMeans\n",
        "# # https://stackoverflow.com/questions/55162989/pyspark-kmeans-clustering-features-column-illegalargumentexception\n",
        "# reduced_df = set_df_columns_nullable(spark, reduced_df, reduced_df.columns)"
      ],
      "metadata": {
        "id": "8TnXTyAnkDAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8leQR4-atMAl"
      },
      "source": [
        "Now run K-means with the same parameters as above, but on the ```svdFeatures``` produced by the SVD reduction you just executed.\n",
        "\n",
        "Compute the Silhouette score, as well as the number of data points that have been clustered correctly."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import dataframe\n",
        "from pyspark.mllib.linalg import Matrix, Matrices, DenseMatrix\n",
        "# Matrix Multiply U by s: result should be either a RowMatrix or a dataframe\n",
        "# svdUsProductRowMatrix or svdUsProductDf\n",
        "# if RowMatrix:\n",
        "#  svdUsProductDf = spark.createDataFrame(pd.DataFrame(svdUsProductRowMatrix))\n",
        "u_factor = svdFeatures.U\n",
        "\n",
        "# convert svd.s[DenseVector] into a numpy.Array\n",
        "# https://api-docs.databricks.com/python/pyspark/latest/api/pyspark.mllib.linalg.DenseVector.html?highlight=densevector#pyspark.mllib.linalg.DenseVector.toArray\n",
        "s_factor_numpyArray = svdFeatures.s.toArray()\n",
        "\n",
        "# convert numpy.Array into a DenseMatrix\n",
        "s_factor_DenseMatrix = DenseMatrix(2, 2, [svdFeatures.s[0], 0, 0, svdFeatures.s[1]])\n",
        "\n",
        "# Multiply U by s\n",
        "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.linalg.distributed.RowMatrix.html#pyspark.mllib.linalg.distributed.RowMatrix.multiply\n",
        "reduced_matrix = u_factor.multiply(s_factor_DenseMatrix)\n",
        "\n",
        "# Convert to pandas df & then spark df\n",
        "reduced_df = spark.createDataFrame(pd.DataFrame(reduced_matrix.rows.collect()))\n",
        "\n",
        "print(\"U: \", u_factor.rows.take(4), \"\\n\")\n",
        "print(\"Σ: \", s_factor_DenseMatrix, \"\\n\")\n",
        "print(\"Final reduced data: \", reduced_df.take(4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNUOSpQloQIR",
        "outputId": "f9c80e7c-345e-48ff-fa42-ca34f83a38ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U:  [DenseVector([0.0728, -0.1402]), DenseVector([0.0771, -0.0229]), DenseVector([0.0683, -0.0048]), DenseVector([0.0227, 0.0009])] \n",
            "\n",
            "Σ:  DenseMatrix([[30786.44462784,     0.        ],\n",
            "             [    0.        ,  2480.44578339]]) \n",
            "\n",
            "Final reduced data:  [Row(0=2241.974276471012, 1=-347.7155601506186), Row(0=2372.4084026672435, 1=-56.90166991262408), Row(0=2101.840279701476, 1=-11.947627371073507), Row(0=697.4321051401307, 1=2.127700026328579)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### DISCARDED CODE #####\n",
        "# Converting the SVD DenseMatrix to a DataFrame bc .fit() needs DF https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.KMeans.html#pyspark.ml.clustering.KMeans.fit\n",
        "# svdFeaturesDenseMatrix = svdFeatures.V\n",
        "# https://stackoverflow.com/a/54106020\n",
        "# denseMatrixToDataFrame = spark.createDataFrame(\n",
        "#     svdFeaturesDenseMatrix.toArray().tolist()\n",
        "# )\n",
        "##########################"
      ],
      "metadata": {
        "id": "PuYLsh_o3Ag-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_snSSj5k2y5"
      },
      "source": [
        "# Code referenced from: https://spark.apache.org/docs/latest/ml-clustering.html#output-columns\n",
        "\n",
        "# Trains a k-means model.\n",
        "svd_reduced_kmeans = KMeans().setK(2).setSeed(1)\n",
        "svd_reduced_model = svd_reduced_kmeans.fit(reduced_df)\n",
        "\n",
        "# Make predictions\n",
        "reduced_predictions = svd_reduced_model.transform(reduced_df)\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "reduced_silhouette = evaluator.evaluate(reduced_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set columns nullable so that rows with invalid data (value == 0) don't create error in kMeans calculation\n",
        "# https://stackoverflow.com/questions/55162989/pyspark-kmeans-clustering-features-column-illegalargumentexception\n",
        "reduced_df = set_df_columns_nullable(spark, reduced_df, reduced_df.columns)\n",
        "\n",
        "# Append reduced features list to reduced df\n",
        "#  KMeans requires features array as column\n",
        "reduced_df = reduced_df.withColumn('features', array(reduced_df.columns))"
      ],
      "metadata": {
        "id": "-WktuFKdrlPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Reduced data Silhouette with squared euclidean distance = \", reduced_silhouette)\n",
        "print(\"Original data Silhouette with squared euclidean distance = \", silhouette)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwvjWjfwz9qs",
        "outputId": "08ab9f0d-fd87-46cf-9fbf-248274f457e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced data Silhouette with squared euclidean distance =  0.8351779819006669\n",
            "Original data Silhouette with squared euclidean distance =  0.8342904262826145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_prediction_series = (reduced_predictions.select('prediction').rdd.map(lambda x: x[0]).collect())\n",
        "print(\"Reduced data labels comparison: \", np.count_nonzero(labels == reduced_prediction_series))\n",
        "print(\"Original data labels comparison: \", np.count_nonzero(labels == prediction_series))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkevInvorQwD",
        "outputId": "aa2cf997-536f-4b08-e938-029bf822d68e"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced data labels comparison:  83\n",
            "Original data labels comparison:  83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5j7crxVMNJG"
      },
      "source": [
        "#### **Submission Intruction:**\n",
        "\n",
        "#### Click File -> Download -> Download .ipynb, and upload the downloaded file to Blackboard."
      ]
    }
  ]
}